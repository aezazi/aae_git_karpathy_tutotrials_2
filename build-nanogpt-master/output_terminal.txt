Running in distributed environment. Initializing FSDP

Running in distributed environment. Initializing FSDP


GPTConfig instantiated with block size: 1024, vocab size: 50304, n_layer: 12, n_head: 12, n_embd: 768

GPTConfig instantiated with block size: 1024, vocab size: 50304, n_layer: 12, n_head: 12, n_embd: 768

FSDP initialized on device: cuda:0, rank: 0, local rank: 0, world size: 2


FSDP initialized on device: cuda:1, rank: 1, local rank: 1, world size: 2

Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3
Rank 0: Managing experts 0 to 1
Rank 1: Managing experts 2 to 3

Total parameters: 237,092,400


Total parameters: 237,092,400


Optimizer initialized on GPU rank 0, device cuda:0

Optimizer initialized on GPU rank 1, device cuda:1

effective batch size desired: 524288
accumulation steps desired: 16

Scheduler initialized on GPU rank 0, of 2


Scheduler initialized on GPU rank 1, of 2

[DEBUG] Rank 0  started training step: 0
[DEBUG] Rank 1  started training step: 0
[DEBUG] Rank 1: Getting gate assignments...
[DEBUG] shape of data to each gpu: [(16384, 768), (16384, 768)]
[DEBUG] Rank 0: Getting gate assignments...
[DEBUG] Rank 1: Gate assignments complete, top_k_ids shape: torch.Size([16384, 2])
[DEBUG] Rank 0: Gate assignments complete, top_k_ids shape: torch.Size([16384, 2])
[DEBUG] Rank 1: Getting expert assignments...
[DEBUG] Rank 0: Getting expert assignments...
[DEBUG] Rank 0: Sending 14539 tokens to GPU 0
[DEBUG] Rank 0: Sending 12612 tokens to GPU 1
----------------------------------------------------------------------
[DEBUG] Rank 0 entered _communicate_tokens
[DEBUG] Rank 0 shape top_k_expert_local_id_assignments_padded : torch.Size([14539, 2])
[DEBUG] Rank 0 top_k_weights_padded: torch.Size([14539, 2])
[DEBUG] Rank 0 shape top_k_expert_local_id_assignments_padded : torch.Size([12612, 2])
[DEBUG] Rank 0 top_k_weights_padded: torch.Size([12612, 2])
[DEBUG] Rank 0 send counts after looping gpus: = [14539, 12612], sum = 27151, tensor_rows = 12612
[DEBUG] Rank 0 input_split_sizes_tensor: tensor([14539, 12612], device='cuda:0', dtype=torch.int32)

[DEBUG] Rank 0 initiating dist.all_to_all_single(recv_counts, send_counts)

[DEBUG] Rank 1: Sending 14727 tokens to GPU 0
[DEBUG] Rank 1: Sending 12586 tokens to GPU 1
----------------------------------------------------------------------
[DEBUG] Rank 1 entered _communicate_tokens
[DEBUG] Rank 1 shape top_k_expert_local_id_assignments_padded : torch.Size([14727, 2])
[DEBUG] Rank 1 top_k_weights_padded: torch.Size([14727, 2])
[DEBUG] Rank 1 shape top_k_expert_local_id_assignments_padded : torch.Size([12586, 2])
[DEBUG] Rank 1 top_k_weights_padded: torch.Size([12586, 2])
[DEBUG] Rank 1 send counts after looping gpus: = [14727, 12586], sum = 27313, tensor_rows = 12586
[DEBUG] Rank 1 input_split_sizes_tensor: tensor([14727, 12586], device='cuda:1', dtype=torch.int32)

[DEBUG] Rank 1 initiating dist.all_to_all_single(recv_counts, send_counts)

[DEBUG] Rank 1 recv_counts: [12612, 12586]
[DEBUG] Rank 0 recv_counts: [14539, 14727]
[DEBUG] Rank 1 N_recv: 25198
[DEBUG] Rank 0 N_recv: 29266

[DEBUG] Rank 1: Token communication complete, received 25198 tokens


[DEBUG] Rank 0: Token communication complete, received 29266 tokens

---------------------------------------------------------------------- ---------------------------------------------------------------------- 



[DEBUG] Rank 1 entered _process_local_experts

[DEBUG] Rank 0 entered _process_local_experts

[DEBUG] Rank 1: Processing 25198 tokens through local experts

[DEBUG] Rank 0: Processing 29266 tokens through local experts

[DEBUG] Rank 1: Input shapes - tokens: torch.Size([25198, 768]), expert_ids: torch.Size([25198, 2]), top_k_weights: torch.Size([25198, 2])

[DEBUG] Rank 0: Input shapes - tokens: torch.Size([29266, 768]), expert_ids: torch.Size([29266, 2]), top_k_weights: torch.Size([29266, 2])

[DEBUG] Rank 1: Starting token-by-token processing...

[DEBUG] Rank 0: Starting token-by-token processing...

[DEBUG] Rank 1: Processing token 0/25198
[DEBUG] Rank 0: Processing token 0/29266
[DEBUG] Rank 0: Processing token 5000/29266
[DEBUG] Rank 1: Processing token 5000/25198
[DEBUG] Rank 0: Processing token 10000/29266
[DEBUG] Rank 1: Processing token 10000/25198
[DEBUG] Rank 0: Processing token 15000/29266
[DEBUG] Rank 1: Processing token 15000/25198
[DEBUG] Rank 0: Processing token 20000/29266
[DEBUG] Rank 1: Processing token 20000/25198
[DEBUG] Rank 0: Processing token 25000/29266
[DEBUG] Rank 1: Processing token 25000/25198

[DEBUG] Rank 1: num  processed tokens: torch.Size([25198, 768])


[DEBUG] Rank 1: tokens_to_send_back_to_gpu_rank  0  shape: torch.Size([14727, 768]) 


[DEBUG] Rank 1: tokens_to_send_back_to_gpu_rank  1  shape: torch.Size([10471, 768]) 


[DEBUG] Rank 0: num  processed tokens: torch.Size([29266, 768])


[DEBUG] Rank 0: tokens_to_send_back_to_gpu_rank  0  shape: torch.Size([14539, 768]) 


[DEBUG] Rank 0: tokens_to_send_back_to_gpu_rank  1  shape: torch.Size([12612, 768]) 

